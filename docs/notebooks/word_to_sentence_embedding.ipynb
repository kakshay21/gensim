{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors, TfidfModel\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from scipy.sparse.linalg import svds\n",
    "from scipy.spatial.distance import cosine\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pre-tained word2vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the pretrained model form [here](https://github.com/RaRe-Technologies/gensim-data/releases/tag/glove-wiki-gigaword-200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = KeyedVectors.load_word2vec_format(\"glove-wiki-gigaword-200.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akshay/Desktop/Incub/env/lib/python2.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(400000, 200)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(word2vec_model.syn0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For doc2vec and sent2vec we'll train the model with [this dataset](http://ai.stanford.edu/~amaas/data/sentiment/).\n",
    "\n",
    "IMDB Dataset: 100,000 movie-reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing with dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total running time: ', 0.001635999999990645)\n"
     ]
    }
   ],
   "source": [
    "import locale\n",
    "import glob\n",
    "import os.path\n",
    "import requests\n",
    "import tarfile\n",
    "import sys\n",
    "import codecs\n",
    "import smart_open\n",
    "\n",
    "dirname = 'aclImdb'\n",
    "filename = 'aclImdb_v1.tar.gz'\n",
    "locale.setlocale(locale.LC_ALL, 'C')\n",
    "\n",
    "if sys.version > '3':\n",
    "    control_chars = [chr(0x85)]\n",
    "else:\n",
    "    control_chars = [unichr(0x85)]\n",
    "\n",
    "# Convert text to lower-case and strip punctuation/symbols from words\n",
    "def normalize_text(text):\n",
    "    norm_text = text.lower()\n",
    "    # Replace breaks with spaces\n",
    "    norm_text = norm_text.replace('<br />', ' ')\n",
    "    # Pad punctuation with spaces on both sides\n",
    "    for char in ['.', '\"', ',', '(', ')', '!', '?', ';', ':']:\n",
    "        norm_text = norm_text.replace(char, ' ' + char + ' ')\n",
    "    return norm_text\n",
    "\n",
    "import time\n",
    "start = time.clock()\n",
    "\n",
    "if not os.path.isfile('aclImdb/alldata-id.txt'):\n",
    "    if not os.path.isdir(dirname):\n",
    "        if not os.path.isfile(filename):\n",
    "            # Download IMDB archive\n",
    "            print(\"Downloading IMDB archive...\")\n",
    "            url = u'http://ai.stanford.edu/~amaas/data/sentiment/' + filename\n",
    "            r = requests.get(url)\n",
    "            with open(filename, 'wb') as f:\n",
    "                f.write(r.content)\n",
    "        tar = tarfile.open(filename, mode='r')\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "    # Concatenate and normalize test/train data\n",
    "    print(\"Cleaning up dataset...\")\n",
    "    folders = ['train/pos', 'train/neg', 'test/pos', 'test/neg', 'train/unsup']\n",
    "    alldata = u''\n",
    "    for fol in folders:\n",
    "        temp = u''\n",
    "        output = fol.replace('/', '-') + '.txt'\n",
    "        # Is there a better pattern to use?\n",
    "        txt_files = glob.glob(os.path.join(dirname, fol, '*.txt'))\n",
    "        for txt in txt_files:\n",
    "            with smart_open.smart_open(txt, \"rb\") as t:\n",
    "                t_clean = t.read().decode(\"utf-8\")\n",
    "                for c in control_chars:\n",
    "                    t_clean = t_clean.replace(c, ' ')\n",
    "                temp += t_clean\n",
    "            temp += \"\\n\"\n",
    "        temp_norm = normalize_text(temp)\n",
    "        with smart_open.smart_open(os.path.join(dirname, output), \"wb\") as n:\n",
    "            n.write(temp_norm.encode(\"utf-8\"))\n",
    "        alldata += temp_norm\n",
    "\n",
    "    with smart_open.smart_open(os.path.join(dirname, 'alldata-id.txt'), 'wb') as f:\n",
    "        for idx, line in enumerate(alldata.splitlines()):\n",
    "            num_line = u\"_*{0} {1}\\n\".format(idx, line)\n",
    "            f.write(num_line.encode(\"utf-8\"))\n",
    "\n",
    "end = time.clock()\n",
    "print (\"Total running time: \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "assert os.path.isfile(\"aclImdb/alldata-id.txt\"), \"alldata-id.txt unavailable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 docs: 25000 train-sentiment, 25000 test-sentiment\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from collections import namedtuple\n",
    "\n",
    "SentimentDocument = namedtuple('SentimentDocument', 'words tags split sentiment')\n",
    "\n",
    "alldocs = []  # Will hold all docs in original order\n",
    "with open('aclImdb/alldata-id.txt') as alldata:\n",
    "    for line_no, line in enumerate(alldata):\n",
    "        tokens = gensim.utils.to_unicode(line).split()\n",
    "        words = tokens[1:]\n",
    "        tags = [line_no] # 'tags = [tokens[0]]' would also work at extra memory cost\n",
    "        split = ['train', 'test', 'extra', 'extra'][line_no//25000]  # 25k train, 25k test, 25k extra\n",
    "        sentiment = [1.0, 0.0, 1.0, 0.0, None, None, None, None][line_no//12500] # [12.5K pos, 12.5K neg]*2 then unknown\n",
    "        alldocs.append(SentimentDocument(words, tags, split, sentiment))\n",
    "\n",
    "train_docs = [doc for doc in alldocs if doc.split == 'train']\n",
    "test_docs = [doc for doc in alldocs if doc.split == 'test']\n",
    "doc_list = alldocs[:]  # For reshuffling per pass\n",
    "\n",
    "print('%d docs: %d train-sentiment, %d test-sentiment' % (len(doc_list), len(train_docs), len(test_docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "import gensim.models.doc2vec\n",
    "from collections import OrderedDict\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise\"\n",
    "\n",
    "doc2vec_model = Doc2Vec(dm=1, dm_concat=1, vector_size=200, window=5, negative=5, hs=0, min_count=2, workers=cores)\n",
    "doc2vec_model.build_vocab(alldocs)\n",
    "models_by_name = OrderedDict((str(model), model) for model in [doc2vec_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akshay/Desktop/Incub/env/lib/python2.7/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from random import sample\n",
    "\n",
    "# For timing\n",
    "from contextlib import contextmanager\n",
    "from timeit import default_timer\n",
    "import time \n",
    "\n",
    "@contextmanager\n",
    "def elapsed_timer():\n",
    "    start = default_timer()\n",
    "    elapser = lambda: default_timer() - start\n",
    "    yield lambda: elapser()\n",
    "    end = default_timer()\n",
    "    elapser = lambda: end-start\n",
    "    \n",
    "def logistic_predictor_from_data(train_targets, train_regressors):\n",
    "    logit = sm.Logit(train_targets, train_regressors)\n",
    "    predictor = logit.fit(disp=0)\n",
    "    # print(predictor.summary())\n",
    "    return predictor\n",
    "\n",
    "def error_rate_for_model(test_model, train_set, test_set, infer=False, infer_steps=3, infer_alpha=0.1, infer_subsample=0.1):\n",
    "    \"\"\"Report error rate on test_doc sentiments, using supplied model and train_docs\"\"\"\n",
    "\n",
    "    train_targets, train_regressors = zip(*[(doc.sentiment, test_model.docvecs[doc.tags[0]]) for doc in train_set])\n",
    "    train_regressors = sm.add_constant(train_regressors)\n",
    "    predictor = logistic_predictor_from_data(train_targets, train_regressors)\n",
    "\n",
    "    test_data = test_set\n",
    "    if infer:\n",
    "        if infer_subsample < 1.0:\n",
    "            test_data = sample(test_data, int(infer_subsample * len(test_data)))\n",
    "        test_regressors = [test_model.infer_vector(doc.words, steps=infer_steps, alpha=infer_alpha) for doc in test_data]\n",
    "    else:\n",
    "        test_regressors = [test_model.docvecs[doc.tags[0]] for doc in test_docs]\n",
    "    test_regressors = sm.add_constant(test_regressors)\n",
    "    \n",
    "    # Predict & evaluate\n",
    "    test_predictions = predictor.predict(test_regressors)\n",
    "    corrects = sum(np.rint(test_predictions) == [doc.sentiment for doc in test_data])\n",
    "    errors = len(test_predictions) - corrects\n",
    "    error_rate = float(errors) / len(test_predictions)\n",
    "    return (error_rate, errors, len(test_predictions), predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "best_error = defaultdict(lambda: 1.0)  # To selectively print only best errors achieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import datetime\n",
    "\n",
    "alpha, min_alpha, passes = (0.025, 0.001, 20)\n",
    "alpha_delta = (alpha - min_alpha) / passes\n",
    "\n",
    "print(\"START %s\" % datetime.datetime.now())\n",
    "\n",
    "for epoch in range(passes):\n",
    "    shuffle(doc_list)  # Shuffling gets best results\n",
    "    \n",
    "    for name, train_model in models_by_name.items():\n",
    "        # Train\n",
    "        duration = 'na'\n",
    "        train_model.alpha, train_model.min_alpha = alpha, alpha\n",
    "        with elapsed_timer() as elapsed:\n",
    "            train_model.train(doc_list, total_examples=len(doc_list), epochs=1)\n",
    "            duration = '%.1f' % elapsed()\n",
    "            \n",
    "        # Evaluate\n",
    "        eval_duration = ''\n",
    "        with elapsed_timer() as eval_elapsed:\n",
    "            err, err_count, test_count, predictor = error_rate_for_model(train_model, train_docs, test_docs)\n",
    "        eval_duration = '%.1f' % eval_elapsed()\n",
    "        best_indicator = ' '\n",
    "        if err <= best_error[name]:\n",
    "            best_error[name] = err\n",
    "            best_indicator = '*' \n",
    "        print(\"%s%f : %i passes : %s %ss %ss\" % (best_indicator, err, epoch + 1, name, duration, eval_duration))\n",
    "\n",
    "        if ((epoch + 1) % 5) == 0 or epoch == 0:\n",
    "            eval_duration = ''\n",
    "            with elapsed_timer() as eval_elapsed:\n",
    "                infer_err, err_count, test_count, predictor = error_rate_for_model(train_model, train_docs, test_docs, infer=True)\n",
    "            eval_duration = '%.1f' % eval_elapsed()\n",
    "            best_indicator = ' '\n",
    "            if infer_err < best_error[name + '_inferred']:\n",
    "                best_error[name + '_inferred'] = infer_err\n",
    "                best_indicator = '*'\n",
    "            print(\"%s%f : %i passes : %s %ss %ss\" % (best_indicator, infer_err, epoch + 1, name + '_inferred', duration, eval_duration))\n",
    "\n",
    "    print('Completed pass %i at alpha %f' % (epoch + 1, alpha))\n",
    "    alpha -= alpha_delta\n",
    "    \n",
    "print(\"END %s\" % str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print best error rates achieved\n",
    "print(\"Err rate Model\")\n",
    "for rate, name in sorted((rate, name) for name, rate in best_error.items()):\n",
    "    print(\"%f %s\" % (rate, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_model.save('doc2vec_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "doc2vec_model = Doc2Vec.load('doc2vec_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Sent2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bizarre', 'horror', 'movie', 'filled', 'famous', 'faces', 'stolen', 'cristina', 'raines', 'later', 'flamingo', 'road', 'pretty', 'somewhat', 'unstable', 'model', 'gummy', 'smile', 'slated', 'pay', 'attempted', 'suicides', 'guarding', 'gateway', 'hell', 'scenes', 'raines', 'modeling', 'captured', 'mood', 'music', 'perfect', 'deborah', 'raffin', 'charming', 'pal', 'raines', 'moves', 'creepy', 'brooklyn', 'heights', 'brownstone', 'inhabited', 'blind', 'priest', 'floor', 'things', 'start', 'cooking', 'neighbors', 'including', 'fantastically', 'wicked', 'burgess', 'meredith', 'kinky', 'couple', 'sylvia', 'miles', 'beverly', 'diabolical', 'lot', 'eli', 'wallach', 'great', 'fun', 'wily', 'police', 'detective', 'movie', 'nearly', 'baby', 'exorcist', 'combination', 'based', 'jeffrey', 'konvitz', 'sentinel', 'entertainingly', 'spooky', 'shocks', 'brought', 'director', 'michael', 'winner', 'mounts', 'thoughtfully', 'downbeat', 'ending', 'skill']\n"
     ]
    }
   ],
   "source": [
    "from gensim import utils\n",
    "import smart_open\n",
    "\n",
    "all_docs = []\n",
    "with smart_open.smart_open('aclImdb/alldata-id.txt') as alldata:\n",
    "    for line_no, line in enumerate(alldata):\n",
    "        tokens = line.split()\n",
    "        remove = ['(', ')', '\"','?','!']\n",
    "        tokens = tokens[1:]\n",
    "        tokens = [word for word in tokens if word not in STOPWORDS]\n",
    "        tokens = [word for word in tokens if word not in remove]\n",
    "        tokens = [word for word in tokens if word.isalpha()]\n",
    "        all_docs.append(tokens)\n",
    "print(all_docs[0])\n",
    "temp = u\"\"\n",
    "with smart_open.smart_open('sent2vec.txt', 'w') as f:\n",
    "    for review in all_docs:\n",
    "        for item in review:\n",
    "            f.write(\"%s \" % item)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 9M words\n",
      "Number of words:  50771\n",
      "Number of labels: 0\n",
      "Progress: 73.8%  words/sec/thread: 34629  lr: 0.052383  loss: 2.594971  eta: 0h1m 3m m %  words/sec/thread: 12652  lr: 0.199283  loss: 4.122703  eta: 0h11m 0.4%  words/sec/thread: 13513  lr: 0.199201  loss: 4.043160  eta: 0h10m 9m 0.5%  words/sec/thread: 15865  lr: 0.198932  loss: 3.867796  eta: 0h9m 0h7m 0.9%  words/sec/thread: 20135  lr: 0.198257  loss: 3.626870  eta: 0h7m m 1.1%  words/sec/thread: 22442  lr: 0.197702  loss: 3.534271  eta: 0h6m h6m 0.196779  loss: 3.430077  eta: 0h5m 1.6%  words/sec/thread: 25071  lr: 0.196734  loss: 3.428211  eta: 0h5m 1.7%  words/sec/thread: 25485  lr: 0.196520  loss: 3.410576  eta: 0h5m   eta: 0h5m 5m 2.5%  words/sec/thread: 27728  lr: 0.195015  loss: 3.319271  eta: 0h5m 0h5m m 29899  lr: 0.192352  loss: 3.221656  eta: 0h4m %  words/sec/thread: 29973  lr: 0.192191  loss: 3.216617  eta: 0h4m 4.1%  words/sec/thread: 30220  lr: 0.191860  loss: 3.208455  eta: 0h4m 4.9%  words/sec/thread: 30792  lr: 0.190283  loss: 3.175084  eta: 0h4m %  words/sec/thread: 30902  lr: 0.190061  loss: 3.171309  eta: 0h4m 31002  lr: 0.189814  loss: 3.166848  eta: 0h4m 5.2%  words/sec/thread: 31038  lr: 0.189504  loss: 3.161499  eta: 0h4m %  words/sec/thread: 31566  lr: 0.188125  loss: 3.137011  eta: 0h4m 6.5%  words/sec/thread: 31830  lr: 0.187065  loss: 3.119431  eta: 0h4m   words/sec/thread: 31879  lr: 0.186826  loss: 3.115182  eta: 0h4m 0h4m 0h4m 7.0%  words/sec/thread: 32068  lr: 0.185954  loss: 3.103741  eta: 0h4m h4m 8.1%  words/sec/thread: 32455  lr: 0.183830  loss: 3.077750  eta: 0h4m 9.2%  words/sec/thread: 32822  lr: 0.181629  loss: 3.054304  eta: 0h4m 32875  lr: 0.181259  loss: 3.049173  eta: 0h4m 4m h4m m 10.8%  words/sec/thread: 33179  lr: 0.178365  loss: 3.020629  eta: 0h3m 11.0%  words/sec/thread: 33200  lr: 0.178010  loss: 3.017607  eta: 0h3m 3.009064  eta: 0h3m %  words/sec/thread: 33310  lr: 0.176146  loss: 2.999549  eta: 0h3m 0.176114  loss: 2.999177  eta: 0h3m 12.8%  words/sec/thread: 33424  lr: 0.174356  loss: 2.985082  eta: 0h3m 0h3m 0h3m 13.6%  words/sec/thread: 33469  lr: 0.172751  loss: 2.972929  eta: 0h3m m 13.7%  words/sec/thread: 33489  lr: 0.172509  loss: 2.971172  eta: 0h3m %  words/sec/thread: 33506  lr: 0.172092  loss: 2.967457  eta: 0h3m h3m 33567  lr: 0.171248  loss: 2.960510  eta: 0h3m 14.7%  words/sec/thread: 33612  lr: 0.170578  loss: 2.955766  eta: 0h3m   lr: 0.168839  loss: 2.945612  eta: 0h3m   words/sec/thread: 33659  lr: 0.168636  loss: 2.944562  eta: 0h3m %  words/sec/thread: 33688  lr: 0.167985  loss: 2.940907  eta: 0h3m 33691  lr: 0.167861  loss: 2.940173  eta: 0h3m 16.2%  words/sec/thread: 33678  lr: 0.167546  loss: 2.938161  eta: 0h3m 0h3m 33839  lr: 0.164324  loss: 2.919840  eta: 0h3m 2.919258  eta: 0h3m   words/sec/thread: 33861  lr: 0.163908  loss: 2.918006  eta: 0h3m 18.4%  words/sec/thread: 33857  lr: 0.163240  loss: 2.914750  eta: 0h3m 0h3m %  words/sec/thread: 33987  lr: 0.160137  loss: 2.900263  eta: 0h3m m 2.893809  eta: 0h3m 0h3m 34069  lr: 0.158243  loss: 2.891801  eta: 0h3m 34108  lr: 0.156698  loss: 2.884584  eta: 0h3m 3m 34111  lr: 0.155756  loss: 2.880540  eta: 0h3m 2.875422  eta: 0h3m 0h3m 2.865352  eta: 0h3m m m 0h3m 34138  lr: 0.149588  loss: 2.855556  eta: 0h3m   loss: 2.854991  eta: 0h3m 0h3m 34163  lr: 0.146838  loss: 2.844416  eta: 0h3m 27.8%  words/sec/thread: 34188  lr: 0.144437  loss: 2.835676  eta: 0h3m h3m   eta: 0h3m   eta: 0h3m 29.6%  words/sec/thread: 34217  lr: 0.140754  loss: 2.818982  eta: 0h3m 29.9%  words/sec/thread: 34231  lr: 0.140300  loss: 2.816875  eta: 0h3m 0h3m 30.2%  words/sec/thread: 34243  lr: 0.139561  loss: 2.814566  eta: 0h3m 30.9%  words/sec/thread: 34270  lr: 0.138194  loss: 2.808337  eta: 0h2m 2m 0h2m   lr: 0.134102  loss: 2.790926  eta: 0h2m 33.2%  words/sec/thread: 34322  lr: 0.133564  loss: 2.788793  eta: 0h2m 0h2m 33.7%  words/sec/thread: 34322  lr: 0.132569  loss: 2.785375  eta: 0h2m 2.783107  eta: 0h2m %  words/sec/thread: 34332  lr: 0.131475  loss: 2.781276  eta: 0h2m 2.780490  eta: 0h2m   words/sec/thread: 34323  lr: 0.130953  loss: 2.779233  eta: 0h2m 34.9%  words/sec/thread: 34324  lr: 0.130143  loss: 2.776721  eta: 0h2m %  words/sec/thread: 34339  lr: 0.128617  loss: 2.771111  eta: 0h2m 0.127582  loss: 2.768002  eta: 0h2m 0.126626  loss: 2.764652  eta: 0h2m h2m %  words/sec/thread: 34352  lr: 0.126489  loss: 2.764382  eta: 0h2m 0h2m 37.6%  words/sec/thread: 34363  lr: 0.124731  loss: 2.758269  eta: 0h2m 0h2m 38.1%  words/sec/thread: 34370  lr: 0.123756  loss: 2.755393  eta: 0h2m 2.750274  eta: 0h2m 0h2m m 0h2m   words/sec/thread: 34407  lr: 0.119242  loss: 2.740207  eta: 0h2m 0h2m   lr: 0.117872  loss: 2.735671  eta: 0h2m 2.735336  eta: 0h2m 41.5%  words/sec/thread: 34448  lr: 0.117059  loss: 2.733358  eta: 0h2m   loss: 2.732172  eta: 0h2m 34455  lr: 0.116133  loss: 2.730898  eta: 0h2m m 34466  lr: 0.114223  loss: 2.726560  eta: 0h2m 43.0%  words/sec/thread: 34468  lr: 0.113962  loss: 2.725904  eta: 0h2m 44.5%  words/sec/thread: 34474  lr: 0.110968  loss: 2.718861  eta: 0h2m 45.4%  words/sec/thread: 34475  lr: 0.109226  loss: 2.714830  eta: 0h2m 45.4%  words/sec/thread: 34475  lr: 0.109211  loss: 2.714749  eta: 0h2m 45.6%  words/sec/thread: 34469  lr: 0.108803  loss: 2.713299  eta: 0h2m %  words/sec/thread: 34470  lr: 0.108450  loss: 2.712553  eta: 0h2m %  words/sec/thread: 34470  lr: 0.108373  loss: 2.712313  eta: 0h2m 45.8%  words/sec/thread: 34468  lr: 0.108361  loss: 2.712266  eta: 0h2m 34460  lr: 0.107928  loss: 2.711211  eta: 0h2m %  words/sec/thread: 34464  lr: 0.107844  loss: 2.711066  eta: 0h2m h2m 2.709096  eta: 0h2m   lr: 0.106879  loss: 2.708815  eta: 0h2m 0h2m 0h2m 0h2m 47.8%  words/sec/thread: 34468  lr: 0.104446  loss: 2.703848  eta: 0h2m 48.4%  words/sec/thread: 34467  lr: 0.103212  loss: 2.701275  eta: 0h2m %  words/sec/thread: 34468  lr: 0.101816  loss: 2.698645  eta: 0h2m m h2m 50.6%  words/sec/thread: 34495  lr: 0.098866  loss: 2.692277  eta: 0h2m   lr: 0.098692  loss: 2.691843  eta: 0h2m 0.098039  loss: 2.690341  eta: 0h2m 51.2%  words/sec/thread: 34495  lr: 0.097675  loss: 2.689669  eta: 0h2m 0.096931  loss: 2.688077  eta: 0h2m 0h2m 2m h2m m 53.5%  words/sec/thread: 34538  lr: 0.092999  loss: 2.679769  eta: 0h2m   lr: 0.092868  loss: 2.679417  eta: 0h1m   loss: 2.679107  eta: 0h1m   loss: 2.679054  eta: 0h1m   eta: 0h1m 1m 0h1m   loss: 2.668019  eta: 0h1m 34553  lr: 0.087969  loss: 2.667980  eta: 0h1m   words/sec/thread: 34553  lr: 0.087816  loss: 2.667677  eta: 0h1m 0.087548  loss: 2.667081  eta: 0h1m h1m 34543  lr: 0.085641  loss: 2.662356  eta: 0h1m %  words/sec/thread: 34547  lr: 0.085273  loss: 2.661594  eta: 0h1m 0h1m 1m 58.7%  words/sec/thread: 34550  lr: 0.082692  loss: 2.655583  eta: 0h1m m   loss: 2.654392  eta: 0h1m   lr: 0.081335  loss: 2.652570  eta: 0h1m 59.7%  words/sec/thread: 34546  lr: 0.080692  loss: 2.651207  eta: 0h1m 60.2%  words/sec/thread: 34548  lr: 0.079637  loss: 2.648968  eta: 0h1m 60.2%  words/sec/thread: 34550  lr: 0.079604  loss: 2.648934  eta: 0h1m 1m 0.075935  loss: 2.641206  eta: 0h1m   eta: 0h1m 63.0%  words/sec/thread: 34584  lr: 0.074006  loss: 2.637357  eta: 0h1m   loss: 2.633855  eta: 0h1m   lr: 0.071160  loss: 2.631097  eta: 0h1m 0.070931  loss: 2.630652  eta: 0h1m   loss: 2.629200  eta: 0h1m 65.5%  words/sec/thread: 34604  lr: 0.068990  loss: 2.626714  eta: 0h1m   lr: 0.068838  loss: 2.626339  eta: 0h1m 34604  lr: 0.068814  loss: 2.626260  eta: 0h1m 65.9%  words/sec/thread: 34605  lr: 0.068222  loss: 2.625035  eta: 0h1m 66.3%  words/sec/thread: 34602  lr: 0.067335  loss: 2.623579  eta: 0h1m 66.4%  words/sec/thread: 34602  lr: 0.067261  loss: 2.623428  eta: 0h1m 0.065603  loss: 2.619790  eta: 0h1m 2.618912  eta: 0h1m m %  words/sec/thread: 34604  lr: 0.060011  loss: 2.608711  eta: 0h1m 70.1%  words/sec/thread: 34603  lr: 0.059801  loss: 2.608279  eta: 0h1m 70.2%  words/sec/thread: 34604  lr: 0.059585  loss: 2.607843  eta: 0h1m 2.607518  eta: 0h1m 0h1m %  words/sec/thread: 34614  lr: 0.056708  loss: 2.602683  eta: 0h1m 72.0%  words/sec/thread: 34620  lr: 0.055993  loss: 2.601315  eta: 0h1m h1m 72.3%  words/sec/thread: 34619  lr: 0.055347  loss: 2.600221  eta: 0h1m h1m   loss: 2.596506  eta: 0h1m   lr: 0.052399  loss: 2.595037  eta: 0h1m Progress: 100.0%  words/sec/thread: 34636  lr: 0.000000  loss: 2.508818  eta: 0h0m m %  words/sec/thread: 34636  lr: 0.050679  loss: 2.591745  eta: 0h1m 0h1m 76.2%  words/sec/thread: 34636  lr: 0.047505  loss: 2.586166  eta: 0h1m h1m 76.5%  words/sec/thread: 34635  lr: 0.046940  loss: 2.585318  eta: 0h1m %  words/sec/thread: 34636  lr: 0.045766  loss: 2.583490  eta: 0h0m 0.044513  loss: 2.581147  eta: 0h0m 2.578665  eta: 0h0m   words/sec/thread: 34629  lr: 0.042810  loss: 2.577953  eta: 0h0m 0.042005  loss: 2.576518  eta: 0h0m 0.041365  loss: 2.575394  eta: 0h0m 80.2%  words/sec/thread: 34622  lr: 0.039596  loss: 2.572326  eta: 0h0m 0m 34621  lr: 0.039022  loss: 2.571060  eta: 0h0m 80.8%  words/sec/thread: 34619  lr: 0.038313  loss: 2.569879  eta: 0h0m 0m   words/sec/thread: 34614  lr: 0.036610  loss: 2.566624  eta: 0h0m 0h0m   eta: 0h0m   words/sec/thread: 34625  lr: 0.033828  loss: 2.561846  eta: 0h0m m 84.2%  words/sec/thread: 34623  lr: 0.031682  loss: 2.557919  eta: 0h0m 84.8%  words/sec/thread: 34624  lr: 0.030464  loss: 2.555926  eta: 0h0m 2.554586  eta: 0h0m 85.8%  words/sec/thread: 34627  lr: 0.028426  loss: 2.552205  eta: 0h0m %  words/sec/thread: 34624  lr: 0.027331  loss: 2.550452  eta: 0h0m 2.549499  eta: 0h0m 34626  lr: 0.026643  loss: 2.549225  eta: 0h0m 86.8%  words/sec/thread: 34627  lr: 0.026452  loss: 2.548883  eta: 0h0m   loss: 2.548826  eta: 0h0m   eta: 0h0m 87.2%  words/sec/thread: 34628  lr: 0.025574  loss: 2.547516  eta: 0h0m 87.3%  words/sec/thread: 34627  lr: 0.025388  loss: 2.547171  eta: 0h0m 0.024939  loss: 2.546505  eta: 0h0m 87.6%  words/sec/thread: 34631  lr: 0.024785  loss: 2.546263  eta: 0h0m   eta: 0h0m 0h0m 88.4%  words/sec/thread: 34628  lr: 0.023138  loss: 2.543412  eta: 0h0m 88.5%  words/sec/thread: 34628  lr: 0.022958  loss: 2.543223  eta: 0h0m 88.6%  words/sec/thread: 34628  lr: 0.022801  loss: 2.543026  eta: 0h0m 88.8%  words/sec/thread: 34627  lr: 0.022447  loss: 2.542586  eta: 0h0m 0.022017  loss: 2.542000  eta: 0h0m 0.021155  loss: 2.540577  eta: 0h0m 89.4%  words/sec/thread: 34622  lr: 0.021143  loss: 2.540546  eta: 0h0m 0h0m 90.5%  words/sec/thread: 34612  lr: 0.018937  loss: 2.537424  eta: 0h0m m 0h0m 0m 93.0%  words/sec/thread: 34619  lr: 0.014083  loss: 2.529807  eta: 0h0m   lr: 0.013418  loss: 2.528763  eta: 0h0m 93.5%  words/sec/thread: 34618  lr: 0.012965  loss: 2.528121  eta: 0h0m 94.2%  words/sec/thread: 34629  lr: 0.011620  loss: 2.526006  eta: 0h0m 2.524314  eta: 0h0m 94.8%  words/sec/thread: 34632  lr: 0.010338  loss: 2.523974  eta: 0h0m   eta: 0h0m 95.1%  words/sec/thread: 34637  lr: 0.009811  loss: 2.523180  eta: 0h0m m 2.520995  eta: 0h0m 34639  lr: 0.008153  loss: 2.520374  eta: 0h0m 0m 96.9%  words/sec/thread: 34644  lr: 0.006176  loss: 2.517312  eta: 0h0m 0h0m 97.4%  words/sec/thread: 34642  lr: 0.005112  loss: 2.515851  eta: 0h0m 97.8%  words/sec/thread: 34643  lr: 0.004482  loss: 2.514889  eta: 0h0m 97.8%  words/sec/thread: 34642  lr: 0.004306  loss: 2.514674  eta: 0h0m 97.9%  words/sec/thread: 34642  lr: 0.004102  loss: 2.514404  eta: 0h0m 34641  lr: 0.003696  loss: 2.513830  eta: 0h0m 34635  lr: 0.000994  loss: 2.510101  eta: 0h0m   lr: 0.000633  loss: 2.509702  eta: 0h0m 100.0%  words/sec/thread: 34636  lr: 0.000029  loss: 2.508851  eta: 0h0m \n"
     ]
    }
   ],
   "source": [
    "! ../sent2vec-master/./fasttext sent2vec -input sent2vec.txt -output my_model -dropoutK 0 -dim 200 -epoch 9 -lr 0.2 -thread 10 -bucket 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sent2vec\n",
    "sent2vec_model = sent2vec.Sent2vecModel()\n",
    "sent2vec_model.load_model('my_model.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Classic LSI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(131056 unique tokens: [u'fawn', u'tsukino', u'woode', u'nunnery', u'sonja']...)\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "\n",
    "dictionary = Dictionary(line.split() for line in open('sent2vec.txt'))\n",
    "print dictionary\n",
    "corpus = [dictionary.doc2bow(text) for text in sentences]\n",
    "MmCorpus.serialize('lsi_model.mm', corpus) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LsiModel\n",
    "lsi_model = LsiModel(corpus,id2word=dictionary)\n",
    "lsi_corpus = lsi_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, u'-0.447*\"infinity\" + -0.447*\"war\" + -0.447*\"eagerly\" + -0.447*\"waiting\" + -0.447*\"avengers\" + 0.000*\"baby\" + -0.000*\"attempted\" + -0.000*\"day\" + -0.000*\"completely\" + -0.000*\"different\"'), (1, u'-0.640*\"dog\" + -0.396*\"sample\" + -0.396*\"cat\" + -0.396*\"sentence\" + -0.245*\"day\" + -0.245*\"cute\" + 0.000*\"baby\" + -0.000*\"skipper\" + 0.000*\"attempted\" + -0.000*\"absolute\"')]\n",
      "<gensim.interfaces.TransformedCorpus object at 0x103c44150>\n"
     ]
    }
   ],
   "source": [
    "print(lsi_model.print_topics(2))\n",
    "print(lsi_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, -1.8280155323464498), (3, 0.811393377776851)]\n",
      "[(2, 1.7320508075688772)]\n",
      "[(1, -1.1297757309528396), (3, -1.3128620634895272)]\n",
      "[(0, -2.2360679774997907)]\n",
      "[(4, -1.4142135623730954)]\n"
     ]
    }
   ],
   "source": [
    "for doc in lsi_corpus:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of mixture methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_average(sent):\n",
    "    sents_emd = []\n",
    "    for s in sent:\n",
    "        sent_emd = []\n",
    "        for w in s:\n",
    "            if w in word2vec_model:\n",
    "                sent_emd.append(word2vec_model[w])\n",
    "        sent_emd_ar = np.array(sent_emd)\n",
    "        sum_ = sent_emd_ar.sum(axis=0)\n",
    "        result = sum_/np.sqrt((sum_**2).sum())\n",
    "        sents_emd.append(result)\n",
    "    return sents_emd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(sent):\n",
    "    word_counter = {}\n",
    "    total_count = 0\n",
    "    no_of_sentences = 0\n",
    "    for s in sent:\n",
    "        for w in s:\n",
    "            if w in word_counter:\n",
    "                word_counter[w] = word_counter[w] + 1\n",
    "            else:\n",
    "                word_counter[w] = 1\n",
    "        total_count = total_count + len(s)\n",
    "        no_of_sentences = no_of_sentences +  1\n",
    "    sents_emd = []\n",
    "    for s in sent:\n",
    "        sent_emd = []\n",
    "        for word in s:\n",
    "            tf = word_counter[word]/float(len(s))\n",
    "            idf = np.log(no_of_sentences/float(1+ word_counter[word]))\n",
    "            try:\n",
    "                emd = tf*idf*word2vec_model[word]\n",
    "                sent_emd.append(emd)\n",
    "            except:\n",
    "                continue\n",
    "        sent_emd = np.array(sent_emd)\n",
    "        sum_ = sent_emd.sum(axis=0)\n",
    "        result = sum_/np.sqrt((sum_**2).sum())\n",
    "        sents_emd.append(result)\n",
    "    return sents_emd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we could use the TFIDF API from gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "\n",
    "def tf_idf_v2(sent):\n",
    "    dct = Dictionary(sent)\n",
    "    corpus = [dct.doc2bow(line) for line in sent]\n",
    "    tf_idf_model = TfidfModel(corpus)\n",
    "    vector = tf_idf_model[corpus]\n",
    "    d = {dct.get(id): value for doc in vector for id, value in doc}\n",
    "    sents_emd = []\n",
    "    no_of_sent = sum(1 for i in sent)\n",
    "    for i in xrange(no_of_sent):\n",
    "        sent_emd = []\n",
    "        for j in xrange(len(sent[i])):\n",
    "            word = sent[i][j]\n",
    "            if word in word2vec_model:\n",
    "                emd = d[word]*word2vec_model[word]\n",
    "                sent_emd.append(emd)\n",
    "        sent_emd_np = np.array(sent_emd)\n",
    "        sum_ = sent_emd_np.sum(axis=0)\n",
    "        result = sum_/np.sqrt((sum_**2).sum())\n",
    "        sents_emd.append(result)\n",
    "    \n",
    "    return sents_emd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_inverse_frequency(sent, a=0.001):\n",
    "    word_counter = {}\n",
    "    sentences = []\n",
    "    total_count = 0\n",
    "    no_of_sentences = 0\n",
    "    for s in sent:\n",
    "        for w in s:\n",
    "            if w in word_counter:\n",
    "                word_counter[w] = word_counter[w] + 1\n",
    "            else:\n",
    "                word_counter[w] = 1\n",
    "        total_count = total_count + len(s)\n",
    "        no_of_sentences = no_of_sentences + 1\n",
    "    sents_emd = []\n",
    "    for s in sent:\n",
    "        sent_emd = []\n",
    "        for word in s:\n",
    "            if word in word2vec_model:\n",
    "                emd = (a/(a + (word_counter[word]/total_count)))*word2vec_model[word]\n",
    "                sent_emd.append(emd)\n",
    "        sum_ = np.array(sent_emd).sum(axis=0)\n",
    "        sentence_emd = sum_/float(no_of_sentences)\n",
    "        sents_emd.append(sentence_emd)\n",
    "    u  = np.array(svds(sents_emd, k=1))\n",
    "    u = u[2]\n",
    "    new_sents_emd = []\n",
    "    for s in sents_emd:\n",
    "        s = s - u.dot(u.transpose())*s\n",
    "        new_sents_emd.append(s)\n",
    "    return new_sents_emd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([['sample', 'sentence', 'cat', 'dog'], ['time', 'computers', 'expensive'], ['day', 'cute', 'dog'], ['eagerly', 'waiting', 'avengers', 'infinity', 'war'], ['completely', 'different']], ['this is a sample sentence with cat and dog', 'there was a time when computers were very expensive', 'one more day with cute dog', 'eagerly waiting for Avengers Infinity War', 'this is a completely different'])\n"
     ]
    }
   ],
   "source": [
    "s1_s = \"this is a sample sentence with cat and dog\"\n",
    "s1 = s1_s.lower().split()\n",
    "s1 = [w for w in s1 if w not in STOPWORDS]\n",
    "s2_s = \"there was a time when computers were very expensive\"\n",
    "s2 = s2_s.lower().split()\n",
    "s2 = [w for w in s2 if w not in STOPWORDS]\n",
    "s3_s = \"one more day with cute dog\"\n",
    "s3 = s3_s.lower().split()\n",
    "s3 = [w for w in s3 if w not in STOPWORDS]\n",
    "s4_s = \"eagerly waiting for Avengers Infinity War\"\n",
    "s4 = s4_s.lower().split()\n",
    "s4 = [w for w in s4 if w not in STOPWORDS]\n",
    "s5_s = \"this is a completely different\"\n",
    "s5 = s5_s.lower().split()\n",
    "s5 = [w for w in s5 if w not in STOPWORDS]\n",
    "sentences = [s1, s2, s3, s4, s5]\n",
    "sentences_s = [s1_s, s2_s, s3_s, s4_s, s5_s,]\n",
    "print(sentences, sentences_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_emd1 = smooth_inverse_frequency(sentences)\n",
    "sentences_emd2 = tf_idf_v2(sentences)\n",
    "sentences_emd3 = simple_average(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmarking with cosine distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIF: 0.308996856213 tfIdf: 0.384991586208 SimAvg: 0.297450304031\n",
      "SIF: 0.592167794704 tfIdf: 0.569366067648 SimAvg: 0.569366067648\n"
     ]
    }
   ],
   "source": [
    "d1 = cosine(sentences_emd1[0],sentences_emd1[2])\n",
    "d2 = cosine(sentences_emd2[0],sentences_emd2[2])\n",
    "d3 = cosine(sentences_emd3[0],sentences_emd3[2])\n",
    "print(\"SIF: {} tfIdf: {} SimAvg: {}\".format(d1, d2, d3))\n",
    "d4 = cosine(sentences_emd1[1],sentences_emd1[3])\n",
    "d5 = cosine(sentences_emd2[1],sentences_emd2[3])\n",
    "d6 = cosine(sentences_emd3[1],sentences_emd3[3])\n",
    "print(\"SIF: {} tfIdf: {} SimAvg: {}\".format(d4, d5, d6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc2vec for s1 and s3: 0.599111407995\n",
      "doc2vec for s1 and s4: 0.928267449141\n"
     ]
    }
   ],
   "source": [
    "doc_d1 = doc2vec_model.infer_vector(s1)\n",
    "doc_d2 = doc2vec_model.infer_vector(s3)\n",
    "print(\"doc2vec for s1 and s3: {}\".format(cosine(doc_d1,doc_d2)))\n",
    "doc_d3 = doc2vec_model.infer_vector(s1)\n",
    "doc_d4 = doc2vec_model.infer_vector(s4)\n",
    "print(\"doc2vec for s1 and s4: {}\".format(cosine(doc_d3,doc_d4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent2vec for s1 and s3 0.374465703964\n",
      "sent2vec for s1 and s4 0.9011329934\n"
     ]
    }
   ],
   "source": [
    "embs_sent2vec = sent2vec_model.embed_sentences(sentences_s)\n",
    "print(\"sent2vec for s1 and s3 {}\".format(cosine(embs_sent2vec[0],embs_sent2vec[2])))\n",
    "print(\"sent2vec for s1 and s4 {}\".format(cosine(embs_sent2vec[0],embs_sent2vec[3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with SICK 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "def load_data(loc='./data/'):\n",
    "    \"\"\"\n",
    "    Load the SICK dataset\n",
    "    \"\"\"\n",
    "    trainA, trainB, devA, devB, testA, testB = [],[],[],[],[],[]\n",
    "    trainS, devS, testS = [],[],[]\n",
    "    with open(os.path.join(loc, 'SICK_test_annotated.txt'), 'rb') as f:\n",
    "        for line in f:\n",
    "            text = line.strip().split('\\t')\n",
    "            testA.append(text[1])\n",
    "            testB.append(text[2])\n",
    "            testS.append(text[3])\n",
    "    testS = [float(s) for s in testS[1:]]\n",
    "\n",
    "    return [testA[1:], testB[1:]], testS\n",
    "\n",
    "def evaluate_sick(model, model_name, evaltest=1):\n",
    "    test, scores = load_data()\n",
    "    if evaltest:\n",
    "            print 'Computing test sentence vectors...'\n",
    "            if model_name == 'sent2vec':\n",
    "                testA = np.array(model.embed_sentences(test[0]))\n",
    "                testB = np.array(model.embed_sentences(test[1]))\n",
    "            elif model_name == 'doc2vec':\n",
    "                testA = np.array([model.infer_vector(example.split(' ')) for example in test[0]])\n",
    "                testB = np.array([model.infer_vector(example.split(' ')) for example in test[1]])\n",
    "            elif model_name == 'word2vec_sif':\n",
    "                testA = smooth_inverse_frequency([example.split(' ') for example in test[0]])\n",
    "                testB = smooth_inverse_frequency([example.split(' ') for example in test[1]])\n",
    "            elif model_name == 'word2vec_tfidf':\n",
    "                testA = tf_idf_v2([example.split(' ') for example in test[0]])\n",
    "                testB = tf_idf_v2([example.split(' ') for example in test[1]])\n",
    "            else:\n",
    "                testA = simple_average([example.split(' ') for example in test[0]])\n",
    "                testB = simple_average([example.split(' ') for example in test[1]])\n",
    "\n",
    "            print 'Computing feature combinations...'\n",
    "            result = []\n",
    "            for i in range(len(testA)):\n",
    "                result.append(5.0*(1 - cosine(testA[i],testB[i])))\n",
    "#             print result\n",
    "\n",
    "            print 'Evaluating...'\n",
    "            pr = pearsonr(result, scores)[0]\n",
    "            print 'Test Pearson: ' + str(pr)\n",
    "            sr = spearmanr(result, scores)[0]\n",
    "            print 'Test Spearman: ' + str(sr)\n",
    "            se = mse(result, scores)\n",
    "            print 'Test MSE: ' + str(se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing test sentence vectors...\n",
      "Computing feature combinations...\n",
      "Evaluating...\n",
      "Test Pearson: 0.6036488695603748\n",
      "Test Spearman: 0.517107076620976\n",
      "Test MSE: 1.886883490561824\n"
     ]
    }
   ],
   "source": [
    "evaluate_sick(word2vec_model,'word2vec') # simple average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing test sentence vectors...\n",
      "Computing feature combinations...\n",
      "Evaluating...\n",
      "Test Pearson: 0.5968195918608273\n",
      "Test Spearman: 0.5098569770117501\n",
      "Test MSE: 1.7127764394605596\n"
     ]
    }
   ],
   "source": [
    "evaluate_sick(word2vec_model,'word2vec_sif') # smooth inverse frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing test sentence vectors...\n",
      "Computing feature combinations...\n",
      "Evaluating...\n",
      "Test Pearson: 0.6386344221803884\n",
      "Test Spearman: 0.5186580421530436\n",
      "Test MSE: 0.977400413016066\n"
     ]
    }
   ],
   "source": [
    "evaluate_sick(word2vec_model,'word2vec_tfidf') # tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing test sentence vectors...\n",
      "Computing feature combinations...\n",
      "Evaluating...\n",
      "Test Pearson: 0.3509808745592773\n",
      "Test Spearman: 0.344297777011621\n",
      "Test MSE: 4.549327606679235\n"
     ]
    }
   ],
   "source": [
    "evaluate_sick(doc2vec_model,'doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing test sentence vectors...\n",
      "Computing feature combinations...\n",
      "Evaluating...\n",
      "Test Pearson: 0.5258587190142471\n",
      "Test Spearman: 0.4542118616338372\n",
      "Test MSE: 1.6227105838354625\n"
     ]
    }
   ],
   "source": [
    "evaluate_sick(sent2vec_model,'sent2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
